name: myexp1Hug
dataset:
  proto: TextDatasetHug
  root: data/mediaca
  src: findings.tok
  tgt: impression.tok
  max_len: 80
  use_bucket: false

model:
  proto: TransformerHug

  encoder:
    proto: BertGenerationEncoder
    hidden_size: 512
    intermediate_size: 2048
    num_hidden_layers: 6
    num_attention_heads: 8
    vocab_size: 8769
    attention_probs_dropout_prob: 0.1
    hidden_dropout_prob: 0.1
    hidden_act: gelu
    initializer_range: 0.02
    layer_norm_eps: 1.e-12
    model_type: bert-generation
    position_embedding_type: absolute
    pad_token_id: 0
    eos_token_id: 1
    bos_token_id: 2
    unk_token_id: 3
    mask_token_id: 4

  decoder:
    proto: BertGenerationEncoderDecoder
    hidden_size: 512
    intermediate_size: 2048
    num_hidden_layers: 6
    num_attention_heads: 8
    attention_probs_dropout_prob: 0.1
    vocab_size: 7167 # impression



train:
  proto: NMTTrainer
  device: cuda
  lr: 0.0001
  weight_decay: 0.00001 #1e-05
  batch_size: 32
  lr_decay_factor: 0.5
  lr_decay_patience: 2
  lr_min: 0.000001
  epochs: 99
  early_stop: 10
  eval_start: 0


validator:
  proto: NMTValidator
  device: cuda
  batch_size: 16
  beam_width: 5
  splits: [dev]

ensemblor:
  proto: NMTEnsemblor
  device: cuda
  batch_size: 16
  beam_width: 5
  splits: [dev]
  mode: all # bestall
